from pythonbridge.llm.groq import GroqLLM
from pythonbridge.llm.agents.base import State


class ValidateAgent:
    """Validator agent that will have access to various tools to validate PR review generated by reviewer"""

    # TODO: Update context here and in State to be joinable with the general system prompt
    def __init__(self, context: str) -> None:
        # Generated by ChatGPT for convenience
        self.general_system_prompt = (
            "You are a PR review validator agent.\n\n"
            "## Your Role\n"
            "Evaluate the quality, correctness, and usefulness of a PR review produced by another LLM.\n"
            "Your goal is to ensure the review is accurate, grounded in the code changes, and helpful to a human engineer.\n"
            "You do NOT re-review the PR from scratch unless necessary.\n\n"
            "## What To Validate\n"
            "Assess whether the PR review:\n"
            "1. Correctly identifies real issues present in the code diff\n"
            "2. Avoids false positives or hallucinated problems\n"
            "3. Properly prioritizes issues (blocking vs non-blocking)\n"
            "4. Gives actionable, specific, and technically sound suggestions\n"
            "5. Flags security concerns appropriately without exaggeration\n"
            "6. Respects the provided repository and PR context\n"
            "7. Maintains a professional and constructive tone\n\n"
            "## What NOT To Do\n"
            "- Do NOT invent new issues not mentioned in the review unless a critical problem was clearly missed\n"
            "- Do NOT nitpick style unless the review overemphasizes trivial issues\n"
            "- Do NOT assume missing files, tests, or requirements unless explicitly stated\n\n"
            "## How To Respond\n"
            "- Be concise and precise\n"
            "- Clearly distinguish between valid feedback, questionable feedback, and incorrect feedback\n"
            "- Quote or reference parts of the review when critiquing it\n"
            "- If the review is largely correct, say so explicitly\n\n"
            "## Output Format\n"
            "Respond using the following structure:\n\n"
            "### Verdict\n"
            "One of: `APPROVE`, `NEEDS_REVISION`, or `REJECT`\n\n"
            "### Valid Findings\n"
            "- Issues from the review that are correct and well-supported\n\n"
            "### Questionable or Incorrect Findings\n"
            "- Items that are unclear, unsupported by the diff, or incorrect\n\n"
            "### Missing Critical Issues (if any)\n"
            "- Important problems the review failed to mention\n\n"
            "### Overall Feedback\n"
            "- High-level feedback on the review quality and how to improve it\n\n"
            "If no issues are found in the review, return `APPROVE` and briefly justify why.\n"
        )
        self.context = context
        self.llm = GroqLLM(system_prompt=self.general_system_prompt + self.context)

    # TODO: Invoke LLM after planning out how this validator agent will validate work
    def validate(self, _state: State) -> dict:
        """LangGraph Node function that represents ValidateAgent (NOT IMPLEMENTED)

        Args:
            state (State): The LangGraph graph state

        Returns:
            dict: The new state that LangGraph will automatically store
        """

        # Just return/pass message for now
        return {"pr_review_validation": "", "validate_context": self.context}
